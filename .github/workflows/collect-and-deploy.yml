name: Collect Data and Deploy Website

on:
  schedule:
    # æ¯å¤© UTC 08:00 åŸ·è¡Œ (è¥¿é›…åœ–æ™‚é–“ 00:00 PST / 01:00 PDT)
    - cron: '0 8 * * *'
  workflow_dispatch:  # å…è¨±æ‰‹å‹•è§¸ç™¼

jobs:
  collect-and-deploy:
    runs-on: ubuntu-latest
    
    permissions:
      contents: write  # å…è¨±æ¨é€åˆ° gh-pages åˆ†æ”¯
    
    env:
      TZ: America/Los_Angeles  # è¥¿é›…åœ–æ™‚å€ (PST/PDT)
    
    steps:
    - name: Checkout repository
      uses: actions/checkout@v4
      with:
        fetch-depth: 0  # å®Œæ•´æ­·å²è¨˜éŒ„
    
    - name: Set up Python
      uses: actions/setup-python@v5
      with:
        python-version: '3.11'
        cache: 'pip'
    
    - name: Install system dependencies
      run: |
        sudo apt-get update
        sudo apt-get install -y redis-server
    
    - name: Install Python dependencies
      run: |
        pip install --upgrade pip
        pip install -r requirements.txt
    
    - name: Start Redis server
      run: |
        sudo systemctl start redis-server
        sudo systemctl status redis-server
    
    - name: Configure GitHub tokens
      run: |
        cat > .env.tokens << EOF
        GITHUB_TOKEN_1=${{ secrets.GH_TOKEN_1 }}
        GITHUB_TOKEN_2=${{ secrets.GH_TOKEN_2 }}
        GITHUB_TOKEN_3=${{ secrets.GH_TOKEN_3 }}
        GITHUB_TOKEN_4=${{ secrets.GH_TOKEN_4 }}
        GITHUB_TOKEN_5=${{ secrets.GH_TOKEN_5 }}
        GITHUB_TOKEN_6=${{ secrets.GH_TOKEN_6 }}
        EOF
        
        echo "GITHUB_TOKEN_1=${{ secrets.GH_TOKEN_1 }}" >> $GITHUB_ENV
        echo "GITHUB_TOKEN_2=${{ secrets.GH_TOKEN_2 }}" >> $GITHUB_ENV
        echo "GITHUB_TOKEN_3=${{ secrets.GH_TOKEN_3 }}" >> $GITHUB_ENV
        echo "GITHUB_TOKEN_4=${{ secrets.GH_TOKEN_4 }}" >> $GITHUB_ENV
        echo "GITHUB_TOKEN_5=${{ secrets.GH_TOKEN_5 }}" >> $GITHUB_ENV
        echo "GITHUB_TOKEN_6=${{ secrets.GH_TOKEN_6 }}" >> $GITHUB_ENV
    
    - name: Check if collection is needed
      id: check_collection
      run: |
        python3 << 'PYTHON_EOF'
        import os
        import json
        import glob
        from datetime import datetime, timedelta
        from zoneinfo import ZoneInfo
        
        SEATTLE_TZ = ZoneInfo("America/Los_Angeles")
        
        # æŸ¥æ‰¾æœ€æ–°çš„é …ç›®æ•¸æ“šæ–‡ä»¶
        project_files = glob.glob('data/seattle_projects_*.json')
        
        if not project_files:
            print("ğŸ“­ No existing data found")
            print("needs_collection=true")
            with open(os.environ['GITHUB_OUTPUT'], 'a') as f:
                f.write('needs_collection=true\n')
            exit(0)
        
        # ç²å–æœ€æ–°çš„æ–‡ä»¶
        latest_file = max(project_files)
        print(f"ğŸ“‚ Found latest data: {latest_file}")
        
        try:
            with open(latest_file, 'r') as f:
                data = json.load(f)
            
            # æª¢æŸ¥æ”¶é›†æ™‚é–“
            collected_at = data.get('collected_at')
            if not collected_at:
                print("âš ï¸  No collection timestamp found")
                print("needs_collection=true")
                with open(os.environ['GITHUB_OUTPUT'], 'a') as f:
                    f.write('needs_collection=true\n')
                exit(0)
            
            # è§£ææ™‚é–“ (è™•ç†å„ç¨®æ ¼å¼)
            if collected_at.endswith('Z'):
                collected_time = datetime.fromisoformat(collected_at.replace('Z', '+00:00'))
            elif '+' in collected_at or collected_at.count('-') > 2:
                collected_time = datetime.fromisoformat(collected_at)
            else:
                # å‡è¨­æ˜¯ naive datetimeï¼Œç•¶ä½œ Seattle æ™‚é–“
                collected_time = datetime.fromisoformat(collected_at).replace(tzinfo=SEATTLE_TZ)
            
            now = datetime.now(SEATTLE_TZ)
            time_diff = now - collected_time
            
            hours_since_collection = time_diff.total_seconds() / 3600
            
            print(f"â° Last collection: {collected_at}")
            print(f"â±ï¸  Time since collection: {hours_since_collection:.1f} hours")
            
            # è™•ç†ç•°å¸¸æƒ…æ³ï¼šè² æ•¸è¡¨ç¤ºæ™‚é–“æˆ³åœ¨æœªä¾†ï¼ˆæ™‚å€éŒ¯èª¤ï¼‰
            if hours_since_collection < 0:
                print(f"âš ï¸  Warning: timestamp is in the future (timezone issue)")
                print(f"ğŸ”„ Treating as old data, collection needed")
                with open(os.environ['GITHUB_OUTPUT'], 'a') as f:
                    f.write('needs_collection=true\n')
            elif hours_since_collection < 24:
                print(f"âœ… Data is fresh (< 24 hours), skipping collection")
                with open(os.environ['GITHUB_OUTPUT'], 'a') as f:
                    f.write('needs_collection=false\n')
            else:
                print(f"ğŸ”„ Data is old (>= 24 hours), collection needed")
                with open(os.environ['GITHUB_OUTPUT'], 'a') as f:
                    f.write('needs_collection=true\n')
        
        except Exception as e:
            print(f"âš ï¸  Error checking data: {e}")
            print("needs_collection=true")
            with open(os.environ['GITHUB_OUTPUT'], 'a') as f:
                f.write('needs_collection=true\n')
        PYTHON_EOF
    
    - name: Run distributed collection
      if: steps.check_collection.outputs.needs_collection == 'true'
      run: |
        echo "ğŸš€ Starting distributed collection..."
        python3 << 'PYTHON_EOF'
        from distributed.distributed_collector import DistributedCollector
        import json
        import os
        from datetime import datetime
        from zoneinfo import ZoneInfo
        
        SEATTLE_TZ = ZoneInfo("America/Los_Angeles")
        
        collector = DistributedCollector(
            batch_size=50,
            auto_manage_workers=True,
            num_workers=8,
            concurrency=2
        )
        
        # Collect data for 30,000 users
        timestamp = datetime.now(SEATTLE_TZ).strftime("%Y%m%d_%H%M%S")
        output_file = f"data/seattle_projects_{timestamp}.json"
        collector.collect(max_users=30000, output_file=output_file)
        
        # Load the collected data
        with open(output_file, 'r') as f:
            data = json.load(f)
        
        # Create ranked_project_local_seattle.json (sorted by score)
        projects = data.get('projects', [])
        
        # Sort by score (stars * 0.6 + forks * 0.3 + watchers * 0.1)
        for proj in projects:
            proj['score'] = (proj.get('stargazers_count', 0) * 0.6 + 
                           proj.get('forks_count', 0) * 0.3 + 
                           proj.get('watchers_count', 0) * 0.1)
        
        projects.sort(key=lambda x: x['score'], reverse=True)
        
        # Save ranked project list
        ranked_output = {
            'total_projects': data.get('total_projects', 0),
            'total_stars': data.get('total_stars', 0),
            'successful_users': data.get('successful_users', 0),
            'filtered_users': data.get('filtered_users', 0),
            'failed_users': data.get('failed_users', 0),
            'collected_at': data.get('collected_at', ''),
            'projects': projects
        }
        
        with open('data/ranked_project_local_seattle.json', 'w') as f:
            json.dump(ranked_output, f, indent=2)
        
        # Create language-based ranking
        language_groups = {}
        for proj in projects:
            lang = proj.get('language') or 'Unknown'
            if lang not in language_groups:
                language_groups[lang] = []
            language_groups[lang].append(proj)
        
        # Save by language
        with open('data/ranked_by_language_seattle.json', 'w') as f:
            json.dump(language_groups, f, indent=2)
        
        print(f"\nâœ… Created ranked_project_local_seattle.json ({len(projects)} projects)")
        print(f"âœ… Created ranked_by_language_seattle.json ({len(language_groups)} languages)")
        
        # Create success marker
        with open('data/.collection_success', 'w') as f:
            f.write(datetime.now(SEATTLE_TZ).isoformat())
        print("âœ… Collection completed successfully")
        PYTHON_EOF
      timeout-minutes: 180  # 3å°æ™‚è¶…æ™‚
    
    - name: Verify or prepare data
      run: |
        if [ "${{ steps.check_collection.outputs.needs_collection }}" == "true" ]; then
          # é©—è­‰æ–°æ”¶é›†çš„æ•¸æ“š
          if [ ! -f "data/.collection_success" ]; then
            echo "âŒ Collection failed - success marker not found"
            exit 1
          fi
          
          if [ ! -f "data/ranked_project_local_seattle.json" ]; then
            echo "âŒ Collection failed - ranked data not found"
            exit 1
          fi
          
          echo "âœ… New collection verification passed"
        else
          # ä½¿ç”¨ç¾æœ‰æ•¸æ“šï¼Œéœ€è¦é‡æ–°ç”Ÿæˆ ranked æ–‡ä»¶
          echo "ğŸ“‚ Using existing data, regenerating ranked files..."
          python3 << 'PYTHON_EOF'
        import json
        import glob
        from datetime import datetime
        from zoneinfo import ZoneInfo
        
        SEATTLE_TZ = ZoneInfo("America/Los_Angeles")
        
        # æŸ¥æ‰¾æœ€æ–°çš„é …ç›®æ•¸æ“šæ–‡ä»¶
        project_files = glob.glob('data/seattle_projects_*.json')
        latest_file = max(project_files)
        
        print(f"ğŸ“‚ Loading data from {latest_file}")
        with open(latest_file, 'r') as f:
            data = json.load(f)
        
        projects = data.get('projects', [])
        
        # æŒ‰ SSR score æ’åº
        projects_sorted = sorted(projects, 
                                key=lambda x: x.get('ssr_score', 0), 
                                reverse=True)
        
        # Save ranked project list
        ranked_output = {
            'total_projects': data.get('total_projects', 0),
            'total_stars': data.get('total_stars', 0),
            'successful_users': data.get('successful_users', 0),
            'filtered_users': data.get('filtered_users', 0),
            'failed_users': data.get('failed_users', 0),
            'collected_at': data.get('collected_at', ''),
            'projects': projects_sorted
        }
        
        with open('data/ranked_project_local_seattle.json', 'w') as f:
            json.dump(ranked_output, f, indent=2)
        
        # Create language-based ranking
        language_groups = {}
        for proj in projects_sorted:
            lang = proj.get('language') or 'Unknown'
            if lang not in language_groups:
                language_groups[lang] = []
            language_groups[lang].append(proj)
        
        # Save by language
        with open('data/ranked_by_language_seattle.json', 'w') as f:
            json.dump(language_groups, f, indent=2)
        
        print(f"âœ… Regenerated ranked_project_local_seattle.json ({len(projects_sorted)} projects)")
        print(f"âœ… Regenerated ranked_by_language_seattle.json ({len(language_groups)} languages)")
        PYTHON_EOF
          
          echo "âœ… Data preparation completed"
        fi
    
    - name: Clean old data files
      run: |
        echo "ğŸ§¹ Cleaning old data files..."
        # ä¿ç•™æœ€æ–°çš„æ–‡ä»¶ï¼Œåˆªé™¤èˆŠçš„
        cd data
        
        # ä¿ç•™ ranked_* æ–‡ä»¶ (å‰›ç”Ÿæˆçš„)
        # åˆªé™¤èˆŠçš„ seattle_projects_*.json (ä¿ç•™æœ€æ–°çš„)
        ls -t seattle_projects_*.json | tail -n +2 | xargs -r rm -v
        
        # åˆªé™¤èˆŠçš„ seattle_users_*.json (ä¿ç•™æœ€æ–°çš„)
        ls -t seattle_users_*.json | tail -n +2 | xargs -r rm -v
        
        # åˆªé™¤æˆåŠŸæ¨™è¨˜æ–‡ä»¶ï¼ˆä¸éœ€è¦æäº¤åˆ° Gitï¼‰
        rm -f .collection_success
        
        echo "âœ… Old data files cleaned"
        echo "ğŸ“ Remaining files:"
        ls -lh *.json
    
    - name: Update README with latest stats
      run: |
        python3 scripts/update_readme.py
    
    - name: Generate frontend paginated data
      run: |
        echo "ğŸ“Š Generating frontend data with topics..."
        python3 scripts/generate_frontend_data.py
        echo "âœ… Frontend data generated with topics support"
    
    - name: Copy data to frontend
      run: |
        # è¤‡è£½æœ€æ–°çš„æ•¸æ“šåˆ° frontend/public
        cp data/ranked_by_language_seattle.json frontend/public/
        cp data/ranked_project_local_seattle.json frontend/public/
        echo "âœ… Data copied to frontend/public"
    
    - name: Set up Node.js
      uses: actions/setup-node@v4
      with:
        node-version: '18'
        cache: 'npm'
        cache-dependency-path: frontend/package-lock.json
    
    - name: Build frontend
      run: |
        cd frontend
        npm ci
        npm run build
    
    - name: Deploy to GitHub Pages
      uses: peaceiris/actions-gh-pages@v3
      with:
        github_token: ${{ secrets.GITHUB_TOKEN }}
        publish_dir: ./frontend/build
        publish_branch: gh-pages
        user_name: 'github-actions[bot]'
        user_email: 'github-actions[bot]@users.noreply.github.com'
        commit_message: 'Deploy: Update with latest data'
    
    - name: Commit updated data
      run: |
        git config user.name "github-actions[bot]"
        git config user.email "github-actions[bot]@users.noreply.github.com"
        
        # Add all changes in data directory (including deletions)
        git add -A data/
        git add README.md
        
        # Check if there are changes
        if git diff --staged --quiet; then
          echo "No changes to commit"
        else
          git commit -m "chore: Update data - $(date +'%Y-%m-%d')"
          git push
        fi
    
    - name: Summary
      if: always()
      run: |
        echo "## Collection Summary" >> $GITHUB_STEP_SUMMARY
        echo "" >> $GITHUB_STEP_SUMMARY
        
        if [ -f data/ranked_project_local_seattle.json ]; then
          USERS=$(jq '.successful_users // 0' data/ranked_project_local_seattle.json)
          PROJECTS=$(jq '.total_projects // 0' data/ranked_project_local_seattle.json)
          STARS=$(jq '.total_stars // 0' data/ranked_project_local_seattle.json)
          
          echo "- ğŸ‘¥ Users: **$USERS**" >> $GITHUB_STEP_SUMMARY
          echo "- ğŸ“¦ Projects: **$PROJECTS**" >> $GITHUB_STEP_SUMMARY
          echo "- â­ Stars: **$STARS**" >> $GITHUB_STEP_SUMMARY
        else
          echo "âš ï¸ Data file not found" >> $GITHUB_STEP_SUMMARY
        fi
