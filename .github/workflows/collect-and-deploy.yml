name: Collect Data and Deploy Website

on:
  schedule:
    # Daily at UTC 08:00 (Seattle 00:00 PST / 01:00 PDT)
    - cron: '0 8 * * *'
  workflow_dispatch:  # Allow manual trigger

jobs:
  collect-and-deploy:
    runs-on: ubuntu-latest
    
    permissions:
      contents: write  # Allow push to gh-pages branch
    
    env:
      TZ: America/Los_Angeles  # Seattle timezone (PST/PDT)
    
    steps:
    - name: Checkout repository
      uses: actions/checkout@v4
      with:
        fetch-depth: 0  # Full history
        token: ${{ secrets.PAT_TOKEN }}  # Use PAT to bypass branch protection
    
    - name: Set up Python
      uses: actions/setup-python@v5
      with:
        python-version: '3.11'
        cache: 'pip'
    
    - name: Install system dependencies
      run: |
        sudo apt-get update
        sudo apt-get install -y redis-server
    
    - name: Install Python dependencies
      run: |
        pip install --upgrade pip
        pip install .
    
    - name: Start Redis server
      run: |
        sudo systemctl start redis-server
        sudo systemctl status redis-server
    
    - name: Configure GitHub tokens
      run: |
        # Create .env.tokens file with all tokens
        cat > .env.tokens << EOF
        GITHUB_TOKEN_1=${{ secrets.GH_TOKEN_1 }}
        GITHUB_TOKEN_2=${{ secrets.GH_TOKEN_2 }}
        GITHUB_TOKEN_3=${{ secrets.GH_TOKEN_3 }}
        GITHUB_TOKEN_4=${{ secrets.GH_TOKEN_4 }}
        GITHUB_TOKEN_5=${{ secrets.GH_TOKEN_5 }}
        GITHUB_TOKEN_6=${{ secrets.GH_TOKEN_6 }}
        EOF
        
        # Export tokens to environment for workers
        echo "GITHUB_TOKEN_1=${{ secrets.GH_TOKEN_1 }}" >> $GITHUB_ENV
        echo "GITHUB_TOKEN_2=${{ secrets.GH_TOKEN_2 }}" >> $GITHUB_ENV
        echo "GITHUB_TOKEN_3=${{ secrets.GH_TOKEN_3 }}" >> $GITHUB_ENV
        echo "GITHUB_TOKEN_4=${{ secrets.GH_TOKEN_4 }}" >> $GITHUB_ENV
        echo "GITHUB_TOKEN_5=${{ secrets.GH_TOKEN_5 }}" >> $GITHUB_ENV
        echo "GITHUB_TOKEN_6=${{ secrets.GH_TOKEN_6 }}" >> $GITHUB_ENV
        
        # Count tokens
        TOKEN_COUNT=$(grep -c "^GITHUB_TOKEN_[0-9]=" .env.tokens)
        echo "[OK] Configured $TOKEN_COUNT GitHub tokens"
    
    - name: Check user data age
      id: check_user_data
      run: |
        python3 << 'PYTHON_EOF'
        import os
        import json
        import glob
        from datetime import datetime
        from zoneinfo import ZoneInfo
        
        SEATTLE_TZ = ZoneInfo("America/Los_Angeles")
        
        # Check user data file (small, committed to Git)
        user_files = glob.glob('data/seattle_users_*.json')
        
        if not user_files:
            print("[ERROR] No user data found")
            print("   â†’ Need full collection (GraphQL + REST API)")
            with open(os.environ['GITHUB_OUTPUT'], 'a') as f:
                f.write('skip_graphql=false\n')
            exit(0)
        
        latest_user_file = max(user_files)
        print(f"ðŸ“ Found user data: {latest_user_file}")
        
        # Parse timestamp from filename (format: seattle_users_YYYYMMDD_HHMMSS.json)
        import os
        filename = os.path.basename(latest_user_file)
        
        try:
            timestamp_str = filename.replace('seattle_users_', '').replace('.json', '')
            date_part, time_part = timestamp_str.split('_')
            
            # Parse date and time from filename
            year = int(date_part[0:4])
            month = int(date_part[4:6])
            day = int(date_part[6:8])
            hour = int(time_part[0:2])
            minute = int(time_part[2:4])
            second = int(time_part[4:6])
            
            # Create datetime with Seattle timezone
            file_time = datetime(year, month, day, hour, minute, second, tzinfo=SEATTLE_TZ)
            now = datetime.now(SEATTLE_TZ)
            hours_since = (now - file_time).total_seconds() / 3600
            
            print(f"ðŸ“… File timestamp: {file_time.isoformat()}")
            print(f"â±ï¸  Hours since: {hours_since:.1f}")
            
            if hours_since < 0:
                print("[WARNING]  Timestamp in future")
                print("   â†’ Need full collection (GraphQL + REST API)")
                with open(os.environ['GITHUB_OUTPUT'], 'a') as f:
                    f.write('skip_graphql=false\n')
            elif hours_since < 24:
                print(f"[OK] User data is fresh (< 24 hours)")
                print("   â†’ Skip GraphQL (reuse existing users)")
                print("   â†’ Only run REST API to collect projects")
                with open(os.environ['GITHUB_OUTPUT'], 'a') as f:
                    f.write('skip_graphql=true\n')
            else:
                print(f"[TIME] User data is old (â‰¥ 24 hours)")
                print("   â†’ Need full collection (GraphQL + REST API)")
                with open(os.environ['GITHUB_OUTPUT'], 'a') as f:
                    f.write('skip_graphql=false\n')
        
        except Exception as e:
            print(f"[ERROR] Error parsing filename: {e}")
            print("   â†’ Need full collection (GraphQL + REST API)")
            with open(os.environ['GITHUB_OUTPUT'], 'a') as f:
                f.write('skip_graphql=false\n')
        PYTHON_EOF
    
    - name: Run distributed collection
      run: |
        echo "[START] Starting distributed collection..."
        python3 << 'PYTHON_EOF'
        from distributed.distributed_collector import DistributedCollector
        import json
        import os
        import sys
        from datetime import datetime
        from zoneinfo import ZoneInfo
        
        SEATTLE_TZ = ZoneInfo("America/Los_Angeles")
        
        # Check if we should skip GraphQL
        skip_graphql = '${{ steps.check_user_data.outputs.skip_graphql }}' == 'true'
        
        if skip_graphql:
            print("=" * 60)
            print("âš¡ FAST MODE: Skipping GraphQL user search")
            print("   Using existing user data from repository")
            print("   Only collecting projects via REST API")
            print("=" * 60)
        else:
            print("=" * 60)
            print("[RETRY] FULL MODE: Running complete collection")
            print("   Step 1: GraphQL user search")
            print("   Step 2: REST API project collection")
            print("=" * 60)
        
        collector = DistributedCollector(
            batch_size=50,
            auto_manage_workers=True,
            num_workers=8,
            concurrency=2
        )
        
        # Collect data for 30,000 users
        timestamp = datetime.now(SEATTLE_TZ).strftime("%Y%m%d_%H%M%S")
        output_file = f"data/seattle_projects_{timestamp}.json"
        
        # The collector will automatically skip GraphQL if user file < 24h exists
        collector.collect(max_users=30000, output_file=output_file)
        
        print(f"[OK] Collection completed: {output_file}")
        
        # Create success marker
        with open('data/.collection_success', 'w') as f:
            f.write(datetime.now(SEATTLE_TZ).isoformat())
        PYTHON_EOF
      timeout-minutes: 180  # 3 hour timeout
    
    - name: Verify collection results
      run: |
        # Verify collected data
        if [ ! -f "data/.collection_success" ]; then
          echo "[ERROR] Collection failed - success marker not found"
          exit 1
        fi
        
        if [ ! -f "data/seattle_projects_"*.json ]; then
          echo "[ERROR] Collection failed - projects data not found"
          exit 1
        fi
        
        echo "[OK] Collection verification passed"
    
    - name: Clean temporary files
      run: |
        echo "ðŸ§¹ Cleaning temporary files..."
        cd data
        
        # Only delete success marker (temporary file)
        rm -f .collection_success
        
        # Note: Keep all seattle_projects_*.json and seattle_users_*.json files
        # They are not pushed to Git anyway (projects too large, users already in repo)
        
        echo "[OK] Old data files cleaned"
        echo "ðŸ“ Remaining files:"
        ls -lh *.json
    
    - name: Update README with latest stats
      run: |
        python3 scripts/update_readme.py
    
    - name: Generate PyPI projects list
      run: |
        echo "[PKG] Checking Python projects on PyPI..."
        python3 scripts/generate_pypi_projects.py
        echo "[OK] PyPI projects list generated"
    
    - name: Update watchers data
      run: |
        echo "[WATCH] Updating watchers data with GraphQL..."
        LATEST_PROJECTS=$(ls -t data/seattle_projects_*.json | head -1)
        echo "Processing: $LATEST_PROJECTS"
        # Start Celery workers
        bash scripts/start_workers.sh
        
        # Run secondary update
        python3 scripts/secondary_update.py "$LATEST_PROJECTS"
        
        # Stop workers
        bash scripts/stop_workers.sh
        echo "[OK] Watchers data updated (validated and enriched)"
    
    - name: Generate frontend paginated data
      run: |
        echo "[STATS] Generating frontend data with topics..."
        python3 scripts/generate_frontend_data.py
        echo "[OK] Frontend data generated with topics support"
    
    - name: Copy PyPI data to frontend
      run: |
        echo "[PKG] Copying PyPI data to frontend..."
        mkdir -p frontend/public/data
        cp data/seattle_pypi_projects.json frontend/public/data/
        echo "[OK] PyPI data copied to frontend/public/data/"
    
    - name: Set up Node.js
      uses: actions/setup-node@v4
      with:
        node-version: '18'
        cache: 'npm'
        cache-dependency-path: frontend/package-lock.json
    
    - name: Build frontend
      run: |
        cd frontend
        npm ci
        npm run build
    
    - name: Deploy to GitHub Pages
      uses: peaceiris/actions-gh-pages@v3
      with:
        github_token: ${{ secrets.GITHUB_TOKEN }}
        publish_dir: ./frontend/build
        publish_branch: gh-pages
        user_name: 'github-actions[bot]'
        user_email: 'github-actions[bot]@users.noreply.github.com'
        commit_message: 'Deploy: Update with latest data'
    
    - name: Commit user data (not projects - too large)
      run: |
        git config user.name "github-actions[bot]"
        git config user.email "github-actions[bot]@users.noreply.github.com"
        
        # Verify the new user file is valid
        echo "[SEARCH] Verifying new user file..."
        LATEST_FILE=$(ls -t data/seattle_users_*.json | head -1)
        echo "Latest file: $LATEST_FILE"
        
        # Check if file exists and is valid JSON
        if [ ! -f "$LATEST_FILE" ]; then
          echo "[ERROR] Error: Latest user file not found"
          exit 1
        fi
        
        # Check user count in the new file
        USER_COUNT=$(jq -r '.total_users // 0' "$LATEST_FILE" 2>/dev/null || echo "0")
        echo "[STATS] New file contains: $USER_COUNT users"
        
        # Only remove old files if new file has sufficient users (at least 20,000)
        if [ "$USER_COUNT" -ge 20000 ]; then
          echo "[OK] Verification passed: $USER_COUNT users (>= 20,000)"
          
          # Remove old user files (keep only the latest one)
          echo "ðŸ§¹ Cleaning up old user files..."
          cd data
          OLD_FILES=$(ls -t seattle_users_*.json | tail -n +2)
          if [ -n "$OLD_FILES" ]; then
            echo "$OLD_FILES" | xargs rm -v
            echo "[OK] Removed $(echo "$OLD_FILES" | wc -l) old user file(s)"
          else
            echo "â„¹ï¸  No old files to remove"
          fi
          cd ..
        else
          echo "[WARNING]  Warning: User count ($USER_COUNT) is below threshold (20,000)"
          echo "   Keeping old user files for safety"
        fi
        
        # Only add user files (small enough for Git)
        # Projects files are too large (252MB) and exceed Git LFS quota
        # Also add README.md with updated stats
        # Add PyPI data files: seattle_pypi_projects.json and pypi_official_packages.json
        git add data/seattle_users_*.json data/seattle_pypi_projects.json data/pypi_official_packages.json README.md
        
        # Check if there are changes
        if git diff --staged --quiet; then
          echo "[OK] No changes to user data, PyPI data, or README"
        else
          echo "ðŸ“¤ Committing updated user data, PyPI data, and README"
          git commit -m "chore: Update user data, PyPI data, and README - $(date +'%Y-%m-%d')"
          git push
        fi
    
    - name: Summary
      if: always()
      run: |
        echo "## Collection Summary" >> $GITHUB_STEP_SUMMARY
        echo "" >> $GITHUB_STEP_SUMMARY
        
        # Find the latest projects file
        LATEST_PROJECTS=$(ls -t data/seattle_projects_*.json 2>/dev/null | head -1)
        
        if [ -f "$LATEST_PROJECTS" ]; then
          # Use -r for raw output and handle potential null/missing fields
          USERS=$(jq -r '.successful_users // 0' "$LATEST_PROJECTS" 2>/dev/null || echo "0")
          PROJECTS=$(jq -r '.total_projects // 0' "$LATEST_PROJECTS" 2>/dev/null || echo "0")
          STARS=$(jq -r '.total_stars // 0' "$LATEST_PROJECTS" 2>/dev/null || echo "0")
          
          echo "- ðŸ‘¥ Users: **$USERS**" >> $GITHUB_STEP_SUMMARY
          echo "- [PKG] Projects: **$PROJECTS**" >> $GITHUB_STEP_SUMMARY
          echo "- â­ Stars: **$STARS**" >> $GITHUB_STEP_SUMMARY
          echo "- ðŸ“ File: \`$(basename $LATEST_PROJECTS)\`" >> $GITHUB_STEP_SUMMARY
        else
          echo "[WARNING] Projects data file not found" >> $GITHUB_STEP_SUMMARY
        fi
        
        # Also show user data info
        LATEST_USERS=$(ls -t data/seattle_users_*.json 2>/dev/null | head -1)
        if [ -f "$LATEST_USERS" ]; then
          USER_COUNT=$(jq -r '.total_users // 0' "$LATEST_USERS" 2>/dev/null || echo "0")
          echo "- [INFO] User accounts collected: **$USER_COUNT**" >> $GITHUB_STEP_SUMMARY
          echo "- ðŸ“„ User file: \`$(basename $LATEST_USERS)\`" >> $GITHUB_STEP_SUMMARY
        fi
        
        # Show PyPI data info
        if [ -f "data/seattle_pypi_projects.json" ]; then
          PYPI_COUNT=$(jq -r '.projects_on_pypi // 0' "data/seattle_pypi_projects.json" 2>/dev/null || echo "0")
          TOTAL_PYTHON=$(jq -r '.total_python_projects // 0' "data/seattle_pypi_projects.json" 2>/dev/null || echo "0")
          DETECTION_RATE=$(jq -r '.detection_rate // "0%"' "data/seattle_pypi_projects.json" 2>/dev/null || echo "0%")
          echo "" >> $GITHUB_STEP_SUMMARY
          echo "### [PKG] PyPI Statistics" >> $GITHUB_STEP_SUMMARY
          echo "- ðŸ Total Python projects: **$TOTAL_PYTHON**" >> $GITHUB_STEP_SUMMARY
          echo "- [PKG] On PyPI: **$PYPI_COUNT** ($DETECTION_RATE)" >> $GITHUB_STEP_SUMMARY
        fi
